# ML Project: Hotel Value Prediction

**Course Project for Kaggle Competition (Checkpoint 1)**

- **Authors:** Manav Jindal (IMT2023535), Arshbir Singh Dang (IMT2023132), Pushkar Kulkarni (IMT2023087)
- **Kaggle Team Name:** Dall-Eminators

---

## Project Overview

This project is a submission for the first checkpoint of our Machine Learning course's Kaggle competition. The objective is to predict the market value (`HotelValue`) of hotel properties based on a dataset of 79 features describing property attributes, location, and amenities.

- **Target Variable:** `HotelValue`
- **Evaluation Metric:** Root Mean Squared Error (RMSE)
- **Model Constraints:** Only models covered in Part 1 of the course were permitted (e.g., Linear Regression, KNN, Decision Trees, and their base ensembles).

## Final Model & Approach

Our best-performing submission is generated by **`07_final_submission_blended.py`**.

This model is a 50/50 blend of two regularized linear models: `RidgeCV` and `LassoCV`. This simple, robust strategy proved to be the most effective, achieving our best score on the Kaggle leaderboard.

The success of this approach was built on a specific preprocessing pipeline:
1.  **Log Transformation:** The target variable `HotelValue` was log-transformed (`np.log1p`) to normalize its right-skewed distribution.
2.  **Imputation:** Missing numerical values were filled with the `median`, and categorical values with the `mode` or `'None'`.
3.  **Feature Engineering:** A few high-impact features were created (e.g., `TotalSF`, `PropertyAge`, `TotalBathrooms`).
4.  **Encoding:** All categorical features were one-hot encoded using `pd.get_dummies`.
5.  **Scaling:** The final feature set was scaled using `StandardScaler`, which is critical for regularized models like Ridge and Lasso.

A key finding of this project (detailed in our report) was that this simple, well-tuned linear approach significantly outperformed more complex tree-based ensembles (like Gradient Boosting, XGBoost, and LightGBM), which were prone to overfitting this dataset.

## How to Reproduce Our Best Submission

To generate our final, best-scoring submission file:

1.  **Prerequisites:**
    * Place the `train.csv` and `test.csv` files from Kaggle into the root of this repository.

2.  **Install Dependencies:**
    ```bash
    pip install pandas numpy scikit-learn
    ```

3.  **Run the Script:**
    ```bash
    python 07_final_submission_blended.py
    ```

4.  **Output:**
    * This will run the complete preprocessing pipeline, train both `RidgeCV` and `LassoCV` models, blend their predictions, and save the result as `submission_champion_blend.csv`.

## Repository Structure & Experimental Journey

This repository contains the full history of our experimental process. The scripts are numbered to tell the story of our project.

- **`Final_Project_Report.pdf`**
  * **This is our main submission document.** It details our full process, from EDA and hypothesis testing to our final conclusions.

- **`01_initial_model_screening.py`**
  * **Start Here:** Our first script, used to test all 9 permitted model types (Linear, KNN, Trees, Ensembles) to find promising candidates.

- **`02_experiment_stacking_ensemble.py`**
  * **Hypothesis 1 (Failed):** An experiment with a complex, 5-model `StackingRegressor` and aggressive feature engineering. This overfit and performed poorly.

- **`03_experiment_xgboost_vs_ridge.py`**
  * **Hypothesis 1 (Failed):** A direct comparison of a tuned `XGBoost` model against our `Ridge` baseline. The linear model performed better.

- **`04_experiment_lightgbm_kfold.py`**
  * **Hypothesis 1 (Failed):** An experiment with `LightGBM` using K-Fold averaging. This also overfit and failed to generalize.

- **`05_linear_model_tuning_baseline.py`**
  * **Hypothesis 2 (Success):** Our first script for the winning strategy. A simple, robust pipeline focused *only* on tuning `Lasso` and `Ridge`.

- **`06_final_linear_model_selection.py`**
  * A refinement of the linear strategy that selects the *single* best model (`Lasso` or `Ridge`) after tuning.

- **`07_final_submission_blended.py`**
  * **WINNING SCRIPT:** The final, polished version of our code. It combines the strengths of both `LassoCV` and `RidgeCV` by blending their predictions, which yielded our best leaderboard score.
