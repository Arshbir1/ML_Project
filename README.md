# ML Project: Hotel Value Prediction

**Course Project for Kaggle Competition (Checkpoint 1)**

- **Authors:** Manav Jindal (IMT2023535), Arshbir Singh Dang (IMT2023132), Pushkar Kulkarni (IMT2023087)
- **Kaggle Team Name:** Dall-Eminators

---

## Project Overview

This project is a submission for the first checkpoint of our Machine Learning course's Kaggle competition. The objective is to predict the market value (`HotelValue`) of hotel properties based on a dataset of 79 features.

- **Target Variable:** `HotelValue`
- **Evaluation Metric:** Root Mean Squared Error (RMSE)
- **Model Constraints:** Only models from Part 1 of the course were permitted.

## Final Model & Approach

Our best-performing submission was generated by the script in the **`10_final_submission_scaled/`** folder.

This model uses a **single, well-tuned linear model** (`Lasso` or `Ridge`) and is built on our most robust and stable preprocessing pipeline.

The success of this approach was built on a specific "simple and scaled" pipeline:
1.  **Log Transformation:** The target variable `HotelValue` was log-transformed (`np.log1p`).
2.  **Imputation:** Missing numerical values were filled with the `median`, and categorical values with the `mode`.
3.  **Simple Feature Engineering:** Only a few high-impact features were created (e.g., `TotalSF`, `PropertyAge`, `TotalBathrooms`).
4.  **Encoding:** All categorical features were one-hot encoded (`pd.get_dummies`).
5.  **Scaling:** The final feature set was scaled using `StandardScaler`, which proved critical for performance.

A key finding (detailed in our report) was that this simple, scaled linear approach significantly outperformed more complex tree-based ensembles (which overfit) and also outperformed more complex linear strategies that used aggressive feature engineering *without* scaling.

## How to Reproduce Our Best Submission

To generate our final, best-scoring submission file:

1.  **Prerequisites:**
    * Place the `train.csv` and `test.csv` files from Kaggle into the root of this repository.

2.  **Install Dependencies:**
    ```bash
    pip install pandas numpy scikit-learn
    ```

3.  **Run the Script:**
    ```bash
    python 10_final_submission_scaled/10_final_linear_model_selection.py
    ```

4.  **Output:**
    * This will run the "simple and scaled" pipeline, select the best model, and save the result as `submission.csv` in the root directory.

## Repository Structure & Experimental Journey

This repository contains the full history of our experimental process. Each numbered folder contains the Python script for that experiment and the `submission.csv` file it generated.

- **`Final_Project_Report.pdf`**
  * **This is our main submission document.** It details our full process, from EDA and hypothesis testing to our final conclusions.

### Hypothesis 1: Advanced Ensembles (Failed)

These experiments correspond to our tests with complex models, which ultimately overfit, as discussed in Section 5.2.1 of our report.

- **`01_initial_screening/`**
  * **Start Here:** Our first script, used to test all 9 permitted model types.
- **`02_stacking_ensemble/`**
  * An experiment with a complex, 5-model `StackingRegressor`.
- **`03_lightgbm_kfold/`**
  * An experiment with `LightGBM` using K-Fold averaging.
- **`04_gbr_tuned/`**
  * Our best-tuned `GradientBoostingRegressor` experiment.
- **`05_xgb_native_api/`**
  * Our best-tuned `XGBoost` experiment using its native API.
- **`06_xgb_vs_ridge/`**
  * A direct comparison of a tuned `XGBoost` (scikit-learn API) against our `Ridge` baseline.

### Hypothesis 2: Robust Linear Models (Winning Strategy)

This was a deep investigation into linear models with different preprocessing strategies.

- **`07_linear_baseline_scaled/`**
  * **The "Simple + Scaled" Strategy:** Our first script for the winning pipeline. Uses simple features and `StandardScaler`.
- **`08_linear_aggressive_blend/`**
  * **The "Aggressive + No-Scale" Strategy:** An experiment with aggressive feature engineering (polynomials, target encoding) and **no scaling**.
- **`09_linear_optimized_blend/`**
  * **The "Optimal Blend" Strategy:** Our most advanced experiment. Uses aggressive features, no scaling, and `scipy.optimize` to find the *optimal* blend weights.

### Final Winning Script

- **`10_final_submission_scaled/`**
  * **WINNING SCRIPT:** This folder contains the final, polished version of our best-performing "Simple + Scaled" strategy. It produced our top score on the Kaggle leaderboard.
